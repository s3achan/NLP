{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6a2cd89-5e2d-48e1-b18f-74d44fe2573c",
   "metadata": {},
   "source": [
    "### ðŸ“š NLTK Gutenberg Corpus â€“ Brief Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15602e0a-2bb6-4b5f-93d5-3350e2ffd382",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#fff9c4; padding:16px; border-radius:8px; border-left:5px solid #fbc02d;\">\n",
    "\n",
    "<h3>ðŸ“˜Executive Summary</h3>\n",
    "\n",
    "The Gutenberg Corpus in NLTK is a collection of classic literary texts from Project Gutenberg, a digital library of public-domain books. It is commonly used for Natural Language Processing (NLP) practice, text analysis, and linguistic research.\n",
    "\n",
    "The corpus includes well-known works such as:\n",
    "<ul>\n",
    "<li>Jane Austen novels (e.g., Emma, Persuasion)</li> \n",
    "<li>Moby Dick by Herman Melville </li>\n",
    "<li>Shakespeare plays </li>\n",
    "<li>The King James Bible </li>\n",
    "</ul>\n",
    "\n",
    "In NLTK, the Gutenberg corpus provides:\n",
    "<ul>\n",
    "<li> Raw text of entire books </li>\n",
    "<li> Easy access to tokenization-ready data</li> \n",
    "<li> A clean environment for experimenting with word frequency, vocabulary richness, sentence length, and other NLP techniques</li>\n",
    "</ul>\n",
    "For the purpose of this analysis, this study utilizes <strong> <em> Austen-Emma</em> </strong> from the Gutenberg corpus.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5f0899-2ea7-4604-88aa-de8f2fd2e356",
   "metadata": {},
   "source": [
    "**Importing the required library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff1782b-6997-48f9-8b59-fa5b00880461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import punkt\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b039d94-71d8-466f-9666-b20c2d0b5a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('gutenberg', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577678c4-8aa7-4f48-806a-00be9057109e",
   "metadata": {},
   "source": [
    "**Loading the book**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f04022-838a-4060-8345-513020393b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Original Text:\n",
      " [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had b\n"
     ]
    }
   ],
   "source": [
    "text = gutenberg.raw('austen-emma.txt')\n",
    "print(\"Sample Original Text:\\n\", text[:600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a76962-a40e-4696-a340-002f78026976",
   "metadata": {},
   "source": [
    "**Tokenize (words + sentence)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af8a4dd-26f2-4988-8360-79db146cd917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences: 7493\n",
      "Total tokens (includes punctuation): 191855\n",
      "\n",
      "Sample sentence:\n",
      " [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "Sample tokens:\n",
      " ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich', ',', 'with', 'a', 'comfortable', 'home']\n"
     ]
    }
   ],
   "source": [
    "sentences = sent_tokenize(text)\n",
    "tokens = word_tokenize(text)\n",
    "print(\"Total sentences:\", len(sentences))\n",
    "print(\"Total tokens (includes punctuation):\", len(tokens))\n",
    "print(\"\\nSample sentence:\\n\", sentences[0])\n",
    "print(\"\\nSample tokens:\\n\", tokens[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afddbb49-4204-4e30-bb78-0802c294305d",
   "metadata": {},
   "source": [
    "**Cleaning the words to lower case & removing non alphanumeric characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4dbfa7e-a65e-4829-a5fe-fda5fc1e3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words only: 157114\n",
      "Sample words: ['emma', 'by', 'jane', 'austen', 'volume', 'i', 'chapter', 'i', 'emma', 'woodhouse', 'handsome', 'clever', 'and', 'rich', 'with', 'a', 'comfortable', 'home', 'and', 'happy']\n"
     ]
    }
   ],
   "source": [
    "words_only = [t.lower() for t in tokens if t.isalpha()]\n",
    "print(\"Words only:\", len(words_only))\n",
    "print(\"Sample words:\", words_only[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0efcd7-dca2-4ca4-b432-7097d803edb1",
   "metadata": {},
   "source": [
    "**World Length Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27094447-f8f2-4e16-96cd-d5c39583fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lengths = [len(w) for w in words_only]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "130e0a69-ee32-4d1e-884b-db185702ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_len = float(np.mean(word_lengths))\n",
    "min_len = int(np.min(word_lengths))\n",
    "max_len = int(np.max(word_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a000cb11-fb9d-4b6f-928e-a49f4134eddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word length stats:\n",
      "  Average: 4.25\n",
      "  Min: 1\n",
      "  Max: 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Word length stats:\")\n",
    "print(\"  Average:\", round(avg_len, 2))\n",
    "print(\"  Min:\", min_len)\n",
    "print(\"  Max:\", max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96104e91-7d2b-4ca2-85e5-504c00d95bc2",
   "metadata": {},
   "source": [
    "**Most common Word Lengths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ee53ece-119e-465d-8e4b-e8b5053e7cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most common word lengths (length -> count):\n",
      "[(3, 37162), (4, 30200), (2, 29527), (5, 16597), (6, 11179), (7, 9734), (1, 6315), (8, 5733), (9, 5314), (10, 2695)]\n"
     ]
    }
   ],
   "source": [
    "len_counts = Counter(word_lengths)\n",
    "print(\"\\nMost common word lengths (length -> count):\")\n",
    "print(len_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c5726f-2d66-4b0d-ba4b-146e35e4bd1f",
   "metadata": {},
   "source": [
    "**Stop Words Removal + Top Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ff0e8d-c77c-40d2-b0e7-f1172f0914d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "content_words = [w for w in words_only if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3904a86d-123f-4052-9806-6542308ecdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stopwords Count: 157114\n",
      "After Stopwords Count: 69693\n"
     ]
    }
   ],
   "source": [
    "print(\"Before Stopwords Count:\", len(words_only))\n",
    "print(\"After Stopwords Count:\", len(content_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35d5cc76-39ff-41dc-89e1-975b550478a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most common words:\n",
      "\n",
      "emma â†’ 860\n",
      "could â†’ 836\n",
      "would â†’ 818\n",
      "miss â†’ 599\n",
      "must â†’ 566\n",
      "harriet â†’ 500\n",
      "much â†’ 484\n",
      "said â†’ 483\n",
      "one â†’ 447\n",
      "weston â†’ 437\n",
      "every â†’ 435\n",
      "thing â†’ 394\n",
      "think â†’ 383\n",
      "elton â†’ 383\n",
      "knightley â†’ 379\n",
      "well â†’ 378\n",
      "little â†’ 359\n",
      "never â†’ 358\n",
      "know â†’ 335\n",
      "might â†’ 325\n"
     ]
    }
   ],
   "source": [
    "freq = FreqDist(content_words)\n",
    "print(\"Top 20 most common words:\\n\")\n",
    "for word, count in freq.most_common(20):\n",
    "    print(f\"{word} â†’ {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89dfd89-4d7a-42b8-b6a8-8076e72c9be8",
   "metadata": {},
   "source": [
    "**Sentence Length Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faa125f7-dba9-4342-96c0-769836710c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_word_count(s: str) -> int:\n",
    "    toks = word_tokenize(s)\n",
    "    toks = [t for t in toks if t.isalpha()]\n",
    "    return len(toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1937b03e-bd43-470a-b3d0-2d1d1c6ff34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_lengths = [sentence_word_count(s) for s in sentences]\n",
    "sentence_lengths = [l for l in sentence_lengths if l > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75bee183-7153-42b4-9f33-a21957ceba9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence length stats (words per sentence):\n",
      "  Count: 7459\n",
      "  Average: 21.06\n",
      "  Median: 15\n",
      "  Min: 1\n",
      "  Max: 234\n"
     ]
    }
   ],
   "source": [
    "print(\"Sentence length stats (words per sentence):\")\n",
    "print(\"  Count:\", len(sentence_lengths))\n",
    "print(\"  Average:\", round(float(np.mean(sentence_lengths)), 2))\n",
    "print(\"  Median:\", int(np.median(sentence_lengths)))\n",
    "print(\"  Min:\", int(np.min(sentence_lengths)))\n",
    "print(\"  Max:\", int(np.max(sentence_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8922338-b558-4748-b75f-73e41083dbb1",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66499dfe-23e1-493d-90e1-5c3c255a2da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Stemmed:\n",
      "emma â†’ emma\n",
      "by â†’ by\n",
      "jane â†’ jane\n",
      "austen â†’ austen\n",
      "volume â†’ volum\n",
      "i â†’ i\n",
      "chapter â†’ chapter\n",
      "i â†’ i\n",
      "emma â†’ emma\n",
      "woodhouse â†’ woodhous\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(w) for w in words_only]\n",
    "print(\"Original vs Stemmed:\")\n",
    "for i in range(10):\n",
    "    print(words_only[i], \"â†’\", stemmed_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "309e7c5c-d092-4e1f-af0d-4d1734df173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Lemmatized:\n",
      "emma â†’ emma\n",
      "by â†’ by\n",
      "jane â†’ jane\n",
      "austen â†’ austen\n",
      "volume â†’ volume\n",
      "i â†’ i\n",
      "chapter â†’ chapter\n",
      "i â†’ i\n",
      "emma â†’ emma\n",
      "woodhouse â†’ woodhouse\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm_tokens = [lemmatizer.lemmatize(w) for w in words_only]\n",
    "print(\"Original vs Lemmatized:\")\n",
    "for i in range(10):\n",
    "    print(words_only[i], \"â†’\", lemm_tokens[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228d028-1cbb-434d-9c7c-6c0b84165b38",
   "metadata": {},
   "source": [
    "**Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22fc3926-19ff-46ab-ada7-bef34282ae83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'characters': 887071,\n",
       " 'sentences': 7493,\n",
       " 'tokens_including_punct': 191855,\n",
       " 'words_only': 157114,\n",
       " 'avg_word_length': 4.25,\n",
       " 'avg_sentence_length_words': 21.06}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = {\n",
    "    \"characters\": len(text),\n",
    "    \"sentences\": len(sentences),\n",
    "    \"tokens_including_punct\": len(tokens),\n",
    "    \"words_only\": len(words_only),\n",
    "    \"avg_word_length\": round(float(np.mean(word_lengths)), 2),\n",
    "    #\"unique_words\": unique_tokens,\n",
    "    \"avg_sentence_length_words\": round(float(np.mean(sentence_lengths)), 2),\n",
    "}\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa4ffde-67dd-4a14-a407-cd16f1cfeb13",
   "metadata": {},
   "source": [
    "**Content Generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7ee7da-1a66-497b-8631-072b864b6150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "felicity wife good marriage you it it of her as narration of but any who an reply by injury but how daily and weymouth has whose but wish walking the did smith her i a a never of set hence henceforward me no shocked it graciously that great vexed one\n"
     ]
    }
   ],
   "source": [
    "def generate_random_text(word_list, length=50):\n",
    "    return \" \".join(random.choice(word_list) for i in range(length))\n",
    "\n",
    "generated_text = generate_random_text(words_only, 50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e155a4c-d642-4615-b297-e3f64753f92d",
   "metadata": {},
   "source": [
    "**POS TAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40a6b033-a8d9-48f3-9b54-7fff09601e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('nlp', 'NN'), ('is', 'VBZ'), ('good', 'JJ'), ('approach', 'NN'), ('for', 'IN'), ('analysis', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"nlp is good approach for analysis\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a3a829-e3df-404b-83cd-2b00615e1c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create 10 quesitons for data set - question & answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a9bdb4-2b49-4eda-9583-afc0ad3c411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent4 = sentences[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb35d3-f1cc-450a-8e0e-f362a0c3df38",
   "metadata": {},
   "source": [
    "**Using Stemmer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa10851f-9748-46a1-ac4f-b73384f30bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma jane austen 1816 volum chapter emma woodhous handsom clever rich comfort home happi disposit seem unit best bless exist live nearli twenti one year world littl distress vex',\n",
       " 'youngest two daughter affection indulg father consequ sister marriag mistress hous earli period',\n",
       " 'mother die long ago indistinct remembr caress place suppli excel woman gover fallen littl short mother affect',\n",
       " 'sixteen year miss taylor mr woodhous famili less gover friend fond daughter particularli emma']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "ps = PorterStemmer()\n",
    "for s in sent4[:4]:\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)  \n",
    "    toks = (w.lower() for w in s.split())\n",
    "    cleaned = \" \".join(ps.stem(w) for w in toks if w not in stop_words)\n",
    "    corpus.append(cleaned)\n",
    "\n",
    "corpus[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b221a3aa-4d1f-41b9-b762-8d9eca08a263",
   "metadata": {},
   "source": [
    "**Using Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7433aa93-0e5d-4643-a3d2-fa04a6579a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma jane austen 1816 volume chapter emma woodhouse handsome clever rich comfortable home happy disposition seemed unite best blessing existence lived nearly twenty one year world little distress vex',\n",
       " 'youngest two daughter affectionate indulgent father consequence sister marriage mistress house early period',\n",
       " 'mother died long ago indistinct remembrance caress place supplied excellent woman governess fallen little short mother affection',\n",
       " 'sixteen year miss taylor mr woodhouse family less governess friend fond daughter particularly emma']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "corpus = []\n",
    "for s in sent4[:4]:\n",
    "    s = re.sub(r\"[^\\w\\s]\", \" \", s)  # remove punctuation\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in s.split() if w.lower() not in stop_words]\n",
    "    corpus.append(\" \".join(words))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912aa9cb-4bfb-459e-a7c7-8ef106bf36cc",
   "metadata": {},
   "source": [
    "**Term Frequency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fc73fbba-0a76-4d57-98d0-881b0eeb1025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'emma': 3,\n",
       "         'woodhouse': 2,\n",
       "         'year': 2,\n",
       "         'little': 2,\n",
       "         'daughter': 2,\n",
       "         'mother': 2,\n",
       "         'governess': 2,\n",
       "         'jane': 1,\n",
       "         'austen': 1,\n",
       "         '1816': 1,\n",
       "         'volume': 1,\n",
       "         'chapter': 1,\n",
       "         'handsome': 1,\n",
       "         'clever': 1,\n",
       "         'rich': 1,\n",
       "         'comfortable': 1,\n",
       "         'home': 1,\n",
       "         'happy': 1,\n",
       "         'disposition': 1,\n",
       "         'seemed': 1,\n",
       "         'unite': 1,\n",
       "         'best': 1,\n",
       "         'blessing': 1,\n",
       "         'existence': 1,\n",
       "         'lived': 1,\n",
       "         'nearly': 1,\n",
       "         'twenty': 1,\n",
       "         'one': 1,\n",
       "         'world': 1,\n",
       "         'distress': 1,\n",
       "         'vex': 1,\n",
       "         'youngest': 1,\n",
       "         'two': 1,\n",
       "         'affectionate': 1,\n",
       "         'indulgent': 1,\n",
       "         'father': 1,\n",
       "         'consequence': 1,\n",
       "         'sister': 1,\n",
       "         'marriage': 1,\n",
       "         'mistress': 1,\n",
       "         'house': 1,\n",
       "         'early': 1,\n",
       "         'period': 1,\n",
       "         'died': 1,\n",
       "         'long': 1,\n",
       "         'ago': 1,\n",
       "         'indistinct': 1,\n",
       "         'remembrance': 1,\n",
       "         'caress': 1,\n",
       "         'place': 1,\n",
       "         'supplied': 1,\n",
       "         'excellent': 1,\n",
       "         'woman': 1,\n",
       "         'fallen': 1,\n",
       "         'short': 1,\n",
       "         'affection': 1,\n",
       "         'sixteen': 1,\n",
       "         'miss': 1,\n",
       "         'taylor': 1,\n",
       "         'mr': 1,\n",
       "         'family': 1,\n",
       "         'less': 1,\n",
       "         'friend': 1,\n",
       "         'fond': 1,\n",
       "         'particularly': 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = \" \".join(corpus).split()\n",
    "tf = Counter(all_words)\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "540c563c-8120-4ba5-837d-572e7e0c6157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c4e1a2-cf31-4f53-9694-77a741e98bd7",
   "metadata": {},
   "source": [
    "**IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5472357d-b457-4e1c-ae7a-2e9458217897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1816</th>\n",
       "      <th>affection</th>\n",
       "      <th>affectionate</th>\n",
       "      <th>ago</th>\n",
       "      <th>austen</th>\n",
       "      <th>best</th>\n",
       "      <th>blessing</th>\n",
       "      <th>caress</th>\n",
       "      <th>chapter</th>\n",
       "      <th>clever</th>\n",
       "      <th>...</th>\n",
       "      <th>twenty</th>\n",
       "      <th>two</th>\n",
       "      <th>unite</th>\n",
       "      <th>vex</th>\n",
       "      <th>volume</th>\n",
       "      <th>woman</th>\n",
       "      <th>woodhouse</th>\n",
       "      <th>world</th>\n",
       "      <th>year</th>\n",
       "      <th>youngest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148070</td>\n",
       "      <td>0.187808</td>\n",
       "      <td>0.148070</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.234126</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.226578</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows Ã— 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       1816  affection  affectionate       ago    austen      best  blessing  \\\n",
       "0  0.187808   0.000000      0.000000  0.000000  0.187808  0.187808  0.187808   \n",
       "1  0.000000   0.000000      0.281477  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000   0.234126      0.000000  0.234126  0.000000  0.000000  0.000000   \n",
       "3  0.000000   0.000000      0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     caress   chapter    clever  ...    twenty       two     unite       vex  \\\n",
       "0  0.000000  0.187808  0.187808  ...  0.187808  0.000000  0.187808  0.187808   \n",
       "1  0.000000  0.000000  0.000000  ...  0.000000  0.281477  0.000000  0.000000   \n",
       "2  0.234126  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     volume     woman  woodhouse     world      year  youngest  \n",
       "0  0.187808  0.000000   0.148070  0.187808  0.148070  0.000000  \n",
       "1  0.000000  0.000000   0.000000  0.000000  0.000000  0.281477  \n",
       "2  0.000000  0.234126   0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000   0.226578  0.000000  0.226578  0.000000  \n",
       "\n",
       "[4 rows x 65 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(binary=False,min_df=1,max_df=1.0,use_idf=True,ngram_range=(1, 1),norm='l2')\n",
    "X = vectorizer.fit_transform(corpus[:4])\n",
    "df = pd.DataFrame(X.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project)",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
