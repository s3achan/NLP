{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bff1782b-6997-48f9-8b59-fa5b00880461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import punkt\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a827ab-8a4e-4f54-b39c-88b0a0926104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b039d94-71d8-466f-9666-b20c2d0b5a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48f04022-838a-4060-8345-513020393b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Original Text:\n",
      " [Emma by Jane Austen 1816]\n",
      "\n",
      "VOLUME I\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "\n",
      "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
      "and happy disposition, seemed to unite some of the best blessings\n",
      "of existence; and had lived nearly twenty-one years in the world\n",
      "with very little to distress or vex her.\n",
      "\n",
      "She was the youngest of the two daughters of a most affectionate,\n",
      "indulgent father; and had, in consequence of her sister's marriage,\n",
      "been mistress of his house from a very early period.  Her mother\n",
      "had died too long ago for her to have more than an indistinct\n",
      "remembrance of her caresses; and her place had b\n"
     ]
    }
   ],
   "source": [
    "text = text = gutenberg.raw('austen-emma.txt')\n",
    "print(\"Sample Original Text:\\n\", text[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af8a4dd-26f2-4988-8360-79db146cd917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 word tokens:\n",
      " ['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']', 'VOLUME', 'I', 'CHAPTER', 'I', 'Emma', 'Woodhouse', ',', 'handsome', ',', 'clever', ',', 'and', 'rich']\n",
      "\n",
      "First 2 sentence tokens:\n",
      " ['[Emma by Jane Austen 1816]\\n\\nVOLUME I\\n\\nCHAPTER I\\n\\n\\nEmma Woodhouse, handsome, clever, and rich, with a comfortable home\\nand happy disposition, seemed to unite some of the best blessings\\nof existence; and had lived nearly twenty-one years in the world\\nwith very little to distress or vex her.', \"She was the youngest of the two daughters of a most affectionate,\\nindulgent father; and had, in consequence of her sister's marriage,\\nbeen mistress of his house from a very early period.\"]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "sent_tokens = sent_tokenize(text)\n",
    "\n",
    "print(\"First 20 word tokens:\\n\", word_tokens[:20])\n",
    "print(\"\\nFirst 2 sentence tokens:\\n\", sent_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c9ad9c1-4fa9-4200-ac0a-6ad8df60af13",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'count_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mword_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount_values\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'count_values'"
     ]
    }
   ],
   "source": [
    "word_tokens.count_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a1bf67-8e2b-41db-ab85-969158da9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text Preview:\n",
      " emma by jane austen volume i chapter i emma woodhouse handsome clever and rich with a comfortable home and happy disposition seemed to unite some of the best blessings of existence and had lived nearly twenty one years in the world with very little to distress or vex her she was the youngest of the two daughters of a most affectionate indulgent father and had in consequence of her sister s marriage been mistress of his house from a very early period her mother had died too long ago for her to have more than an indistinct remembrance of her caresses and her place had been supplied by an excelle\n"
     ]
    }
   ],
   "source": [
    "clean_text = text.lower()                                     # lowercase\n",
    "clean_text = re.sub(r'[^a-z\\s]', ' ', clean_text)             # keep only alphabets\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text).strip()          # remove extra spaces\n",
    "print(\"Cleaned Text Preview:\\n\", clean_text[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3904a86d-123f-4052-9806-6542308ecdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stopwords Count: 161975\n",
      "After Stopwords Count: 73292\n",
      "Sample tokens: ['emma', 'jane', 'austen', 'volume', 'chapter', 'emma', 'woodhouse', 'handsome', 'clever', 'rich', 'comfortable', 'home', 'happy', 'disposition', 'seemed', 'unite', 'best', 'blessings', 'existence', 'lived']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokens_clean = clean_text.split()\n",
    "filtered_tokens = [w for w in tokens_clean if w not in stop_words]\n",
    "\n",
    "print(\"Before Stopwords Count:\", len(tokens_clean))\n",
    "print(\"After Stopwords Count:\", len(filtered_tokens))\n",
    "print(\"Sample tokens:\", filtered_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35d5cc76-39ff-41dc-89e1-975b550478a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most common words:\n",
      "\n",
      "mr → 1154\n",
      "emma → 865\n",
      "could → 837\n",
      "would → 821\n",
      "mrs → 701\n",
      "miss → 602\n",
      "must → 571\n",
      "harriet → 506\n",
      "much → 486\n",
      "said → 484\n",
      "one → 458\n",
      "weston → 440\n",
      "every → 435\n",
      "well → 403\n",
      "thing → 399\n",
      "knightley → 389\n",
      "elton → 387\n",
      "think → 384\n",
      "little → 361\n",
      "never → 358\n"
     ]
    }
   ],
   "source": [
    "freq = FreqDist(filtered_tokens)\n",
    "print(\"Top 20 most common words:\\n\")\n",
    "for word, count in freq.most_common(20):\n",
    "    print(f\"{word} → {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66499dfe-23e1-493d-90e1-5c3c255a2da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Stemmed:\n",
      "emma → emma\n",
      "jane → jane\n",
      "austen → austen\n",
      "volume → volum\n",
      "chapter → chapter\n",
      "emma → emma\n",
      "woodhouse → woodhous\n",
      "handsome → handsom\n",
      "clever → clever\n",
      "rich → rich\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens]\n",
    "print(\"Original vs Stemmed:\")\n",
    "for i in range(10):\n",
    "    print(filtered_tokens[i], \"→\", stemmed_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "309e7c5c-d092-4e1f-af0d-4d1734df173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Lemmatized:\n",
      "emma → emma\n",
      "jane → jane\n",
      "austen → austen\n",
      "volume → volume\n",
      "chapter → chapter\n",
      "emma → emma\n",
      "woodhouse → woodhouse\n",
      "handsome → handsome\n",
      "clever → clever\n",
      "rich → rich\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "lemm_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "print(\"Original vs Lemmatized:\")\n",
    "for i in range(10):\n",
    "    print(filtered_tokens[i], \"→\", lemm_tokens[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project)",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
