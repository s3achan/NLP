{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4705292-d89a-4ab7-b990-25641808be9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in c:\\users\\shikshya\\anaconda3\\envs\\project\\lib\\site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\shikshya\\anaconda3\\envs\\project\\lib\\site-packages (from nltk) (1.5.3)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2026.2.19-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shikshya\\anaconda3\\envs\\project\\lib\\site-packages (from nltk) (4.67.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\shikshya\\anaconda3\\envs\\project\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 16.0 MB/s  0:00:00\n",
      "Downloading regex-2026.2.19-cp310-cp310-win_amd64.whl (277 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   -------------------- ------------------- 1/2 [nltk]\n",
      "   ---------------------------------------- 2/2 [nltk]\n",
      "\n",
      "Successfully installed nltk-3.9.2 regex-2026.2.19\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6b047a5-6783-4e32-81b4-89abc4a7f085",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bff1782b-6997-48f9-8b59-fa5b00880461",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk import punkt\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1a827ab-8a4e-4f54-b39c-88b0a0926104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b039d94-71d8-466f-9666-b20c2d0b5a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Shikshya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48f04022-838a-4060-8345-513020393b3e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'punkt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mpunkt\u001b[49m\u001b[38;5;241m.\u001b[39mwords())\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample Original Text:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, text[:\u001b[38;5;241m600\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'punkt' is not defined"
     ]
    }
   ],
   "source": [
    "text = \" \".join(gutenburg.words())\n",
    "print(\"Sample Original Text:\\n\", text[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8af8a4dd-26f2-4988-8360-79db146cd917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 word tokens:\n",
      " ['plot', ':', 'two', 'teen', 'couples', 'go', 'to', 'a', 'church', 'party', ',', 'drink', 'and', 'then', 'drive', '.', 'they', 'get', 'into', 'an']\n",
      "\n",
      "First 2 sentence tokens:\n",
      " ['plot : two teen couples go to a church party , drink and then drive .', 'they get into an accident .']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "sent_tokens = sent_tokenize(text)\n",
    "\n",
    "print(\"First 20 word tokens:\\n\", word_tokens[:20])\n",
    "print(\"\\nFirst 2 sentence tokens:\\n\", sent_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05a1bf67-8e2b-41db-ab85-969158da9fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text Preview:\n",
      " plot two teen couples go to a church party drink and then drive they get into an accident one of the guys dies but his girlfriend continues to see him in her life and has nightmares what s the deal watch the movie and sorta find out critique a mind fuck movie for the teen generation that touches on a very cool idea but presents it in a very bad package which is what makes this review an even harder one to write since i generally applaud films which attempt to break the mold mess with your head and such lost highway memento but there are good and bad ways of making all types of films and these \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "clean_text = text.lower()                                     # lowercase\n",
    "clean_text = re.sub(r'[^a-z\\s]', ' ', clean_text)             # keep only alphabets\n",
    "clean_text = re.sub(r'\\s+', ' ', clean_text).strip()          # remove extra spaces\n",
    "\n",
    "print(\"Cleaned Text Preview:\\n\", clean_text[:600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3904a86d-123f-4052-9806-6542308ecdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stopwords Count: 1331272\n",
      "After Stopwords Count: 702479\n",
      "Sample tokens: ['plot', 'two', 'teen', 'couples', 'go', 'church', 'party', 'drink', 'drive', 'get', 'accident', 'one', 'guys', 'dies', 'girlfriend', 'continues', 'see', 'life', 'nightmares', 'deal']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "tokens_clean = clean_text.split()\n",
    "filtered_tokens = [w for w in tokens_clean if w not in stop_words]\n",
    "\n",
    "print(\"Before Stopwords Count:\", len(tokens_clean))\n",
    "print(\"After Stopwords Count:\", len(filtered_tokens))\n",
    "print(\"Sample tokens:\", filtered_tokens[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35d5cc76-39ff-41dc-89e1-975b550478a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most common words:\n",
      "\n",
      "film → 9519\n",
      "one → 5854\n",
      "movie → 5775\n",
      "like → 3691\n",
      "even → 2565\n",
      "good → 2411\n",
      "time → 2411\n",
      "story → 2170\n",
      "would → 2110\n",
      "much → 2050\n",
      "character → 2020\n",
      "also → 1967\n",
      "get → 1949\n",
      "two → 1912\n",
      "well → 1906\n",
      "characters → 1859\n",
      "first → 1836\n",
      "see → 1749\n",
      "way → 1693\n",
      "make → 1642\n"
     ]
    }
   ],
   "source": [
    "freq = FreqDist(filtered_tokens)\n",
    "\n",
    "print(\"Top 20 most common words:\\n\")\n",
    "for word, count in freq.most_common(20):\n",
    "    print(f\"{word} → {count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66499dfe-23e1-493d-90e1-5c3c255a2da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Stemmed:\n",
      "plot → plot\n",
      "two → two\n",
      "teen → teen\n",
      "couples → coupl\n",
      "go → go\n",
      "church → church\n",
      "party → parti\n",
      "drink → drink\n",
      "drive → drive\n",
      "get → get\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = [stemmer.stem(w) for w in filtered_tokens]\n",
    "\n",
    "print(\"Original vs Stemmed:\")\n",
    "for i in range(10):\n",
    "    print(filtered_tokens[i], \"→\", stemmed_tokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "309e7c5c-d092-4e1f-af0d-4d1734df173b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Lemmatized:\n",
      "plot → plot\n",
      "two → two\n",
      "teen → teen\n",
      "couples → couple\n",
      "go → go\n",
      "church → church\n",
      "party → party\n",
      "drink → drink\n",
      "drive → drive\n",
      "get → get\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemm_tokens = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "\n",
    "print(\"Original vs Lemmatized:\")\n",
    "for i in range(10):\n",
    "    print(filtered_tokens[i], \"→\", lemm_tokens[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (project)",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
